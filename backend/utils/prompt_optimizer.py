import os
import logging
import google.generativeai as genai
from dotenv import load_dotenv
from typing import Optional, List
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.llms.base import BaseLLM
from langchain.schema import LLMResult, Generation

# Load environment variables
load_dotenv()

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.DEBUG)

# Set up Google Generative AI API key
GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")  # Ensure this is set in .env
if not GOOGLE_API_KEY:
    logger.error("GOOGLE_API_KEY is not set in the environment variables.")
    raise ValueError("GOOGLE_API_KEY is required.")

genai.configure(api_key=GOOGLE_API_KEY)

class GoogleGenerativeAI(BaseLLM):
    """
    A custom LLM class for integrating Google Generative AI (Gemini) with LangChain.
    """
    @property
    def _llm_type(self) -> str:
        return "google_generative_ai"

    def _call(self, prompt: str, stop: Optional[List[str]] = None) -> str:
        """
        Calls the Google Generative AI API to generate text from the given prompt.
        """
        try:
            # Generate content using generate_content
            response = genai.generate_content(
                model="models/text-bison-001", # Specify the model!
                prompt=prompt,
                temperature=0.0,  # Adjust as needed
                max_output_tokens=1024,  # Adjust as needed
            )

            # Log the full response for debugging
            logger.debug(f"Response from Google Generative AI: {response}")

            # Extract the generated text.  Handle potential empty responses
            if response.candidates:
                optimized_text = response.candidates[0].output
                return optimized_text
            else:
                return "No text generated by the model."

        except Exception as e:
            logger.error(f"Error optimizing prompt: {e}")
            return f"Error optimizing prompt: {e}"

    def _generate(
        self, 
        prompts: List[str], 
        stop: Optional[List[str]] = None
    ) -> LLMResult:
        """
        Implements the _generate method required by BaseLLM.
        """
        generations = []
        for prompt in prompts:
            try:
                generated_text = self._call(prompt, stop=stop)
                generations.append([Generation(text=generated_text)])
            except Exception as e:
                logger.error(f"Error generating text for prompt '{prompt}': {e}")
                generations.append([Generation(text=f"Error generating text: {e}")])
        
        return LLMResult(generations=generations, llm_output={})


def optimize_prompt_text(original_prompt: str) -> str:
    """
    Optimizes the given prompt for clarity and effectiveness using Google's Generative AI.
    """
    try:
        llm = GoogleGenerativeAI()
        prompt_template = PromptTemplate(
            input_variables=["original_prompt"],
            template="Optimize the following prompt for clarity and effectiveness:\n\n{original_prompt}"
        )
        chain = LLMChain(llm=llm, prompt=prompt_template)
        optimized_text = chain.run(original_prompt).strip()
        logger.debug(f"Optimized Prompt: {optimized_text}")
        return optimized_text
    except Exception as e:
        logger.error(f"Error optimizing prompt: {str(e)}")
        return f"Error optimizing prompt: {str(e)}"